{"meta":{"title":"John-Yao's Blog","subtitle":null,"description":null,"author":"John Yao","url":"http://yoursite.com","root":"/"},"pages":[{"title":"","date":"2020-08-20T16:14:19.426Z","updated":"2020-08-20T13:26:34.656Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2020-08-20T13:26:34.655Z","updated":"2020-08-20T13:26:34.655Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Aboutme","slug":"Aboutme","date":"2020-08-20T16:34:12.000Z","updated":"2020-08-20T23:30:44.585Z","comments":true,"path":"2020/08/21/Aboutme/","link":"","permalink":"http://yoursite.com/2020/08/21/Aboutme/","excerpt":"","text":"My main research includes object detection, multi-object tracking, image retrieval, pedestrian reidentification. I also achieve some top rank of CVPR, ICCV and Tianchi. Competition AwardMulti-Object-Tracking July 15, 2020 2nd place of Visdrone 2020 Multi-Ojbect-Tracking August 15, 2020 1st place of Gigavision 2020 Pedestrian Tracking Image Retrieval August 17, 2020 Gold medel of Google Landmark Retrieval","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2020-08-20T13:26:34.653Z","updated":"2020-08-20T13:26:34.653Z","comments":true,"path":"2020/08/20/hello-world/","link":"","permalink":"http://yoursite.com/2020/08/20/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Data Augmentations in Image Classification","slug":"Data-Augmentations-in-Image-Classification","date":"2020-04-30T09:10:04.000Z","updated":"2020-08-20T13:26:34.643Z","comments":true,"path":"2020/04/30/Data-Augmentations-in-Image-Classification/","link":"","permalink":"http://yoursite.com/2020/04/30/Data-Augmentations-in-Image-Classification/","excerpt":"","text":"Overview​ 在图像分类中，数据增强是必不可少的步骤，合适的数据增强可以增加模型的泛化能力，取得更好的分类精度。本文旨在整个在图像分类任务上最近一些比较实用的方法，并给出简单的实现过程，在具体的实验中，并不能确保哪种数据增强策略是比较理想的，需要在实际的数据中合适的选择。 ​ 本文主要涉及以下数据增强策略： Cutout DeVries, Terrance, and Graham W. Taylor. “Improved Regularization of Convolutional Neural Networks with Cutout.” arXiv preprint arXiv:1708.04552 (2017). arXiv:1708.04552, PyTorch implementation RandomErase Zhong, Zhun, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. “Random Erasing Data Augmentation.” arXiv preprint arXiv:1708.04896 (2017). arXiv:1708.04896, PyTorch implementation AutoAug Cubuk E D , Zoph B , Mane D , et al. AutoAugment: Learning Augmentation Policies from Data[J]. 2018. Mixup Zhang, Hongyi, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. “mixup: Beyond Empirical Risk Minimization.” In International Conference on Learning Representations (ICLR), 2017. link, arXiv:1710.09412 Zhang Z, He T, Zhang H, et al. Bag of freebies for training object detection neural networks[J]. arXiv preprint arXiv:1902.04103, 2019. Cutmix Yun, Sangdoo, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. “CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features.” arXiv preprint arXiv:1905.04899 (2019). arXiv:1905.04899 RICAP Takahashi, Ryo, Takashi Matsubara, and Kuniaki Uehara. “Data Augmentation using Random Image Cropping and Patching for Deep CNNs.” Proceedings of The 10th Asian Conference on Machine Learning (ACML), 2018. link, arXiv:1811.09030 Cutout ​ Cutout的具体实现效果如上图所示，使用固定大小的正方形的patch在零均值化的图像上随机擦除，擦除的位置变为0. 原文指出，在超参设置上cutout的面积大小比形状更重要，并且在实验过程中发现通过按照50%的比例使用cutout和原图训练的模型往往取得更好的效果。 ​ Cutout变种有：Dual-Cutout，RandomErase RandomErase RandomErase的具体实现效果如上图所示，对比cutout而言，引入了形状、填充值等变化，不是简单的使用方形，而是使用了不同长宽比进行擦除，擦除的填充值采用了随机值，一般实现上使用图像的均值效果差别不大。 RandomErase原文不仅在图像分类进行了实验，在ReID任务上也进行了比较，提分比较明显。 AutoAugAutoAug定义了一些涉及仿射变换、旋转等子策略，通过搜索算法在目标数据集上搜索对应得概率和幅值。在竞赛时间、数据量大、资源有限得情况下，我们可以使用Imagenet上学习得到得子策略进行增强(增强效果如下图所示)。这种增强往往并不一定有效，比如之前在我们的一个数据集上，seresnext101_32x8d使用autoaug反而掉点，但是在使用resnest101的实验中，涨幅十分明显，并且超过了最优的seresnext101_32x8d（对比没使用autoaug的配置，resnest101比seresnext101更低）。因此有时候，有必要组合多种条件进行调参。 Mixupmixup会在一个batch内随机选择两张图片（A,B）进行融合，融合的权重λ从beta分布采样得到，然后对两张图像的标签分别计算损失，并使用λ，1-λ进行加权求和。具体效果可以看cutmix小节的效果图。 mixup在yolov3系列中检测也经常用到,具体可参考Bag of freebies for training object detection neural networks Cutmix如图所示，Cutmix在具体实现上会在一个batch内随机选择两张图片（A,B）进行融合，具体的，以其中一张A作为画布，并使用（0-1）均匀分布选择融合的面积比lambda，从另外一张图片B随机裁剪与其一样长宽比的patch,随机的黏贴到A上。在loss的计算上，按照校准后 1-lambda, lambda 的比例计算对应的分类损失即可 RICAPRICAP全称Random Image Cropping and Patching, RICAP会在会在一个batch内随机选择四张图片（A,B,C,D）进行融合,具体地，从beta分布采样ratio_x,ratio_y，根据预定义好的图像尺寸生成中间点（w，h）,然后再随机从其他图像裁剪一块图像黏贴到对应位置，在损失函数计算上，更加采样窗口的比例进行加权求和 ricap在检测中有个类似的应用Stitcher（Sitcher: Feedback-driven Data Provider for Object Detection ） 后话本文列举的数据增强并不算多，但是如何用好这些需要图像算法工程师的仔细调参，比如在我们的一个数据集中，我们进行了模型resnest和senext的对比实验，加入了autaug对resnest的提升比较大，对senext提升并不明显；在cutmix和ricap的实验中，使用小batchsize的RICAP效果更理想，使用大batchsize的cutmix效果更优，当然这可能跟数据集有关，但是不可否认的是数据增强策略和模型及其超参可能存在一定的耦合，如果是竞赛的话，可以尝试多种组合策略。 参考：本文的主要参考内容如下： https://github.com/hysts/pytorch_image_classification","categories":[{"name":"Image Classification","slug":"Image-Classification","permalink":"http://yoursite.com/categories/Image-Classification/"}],"tags":[{"name":"Image Classification, Data Augmentation","slug":"Image-Classification-Data-Augmentation","permalink":"http://yoursite.com/tags/Image-Classification-Data-Augmentation/"}]},{"title":"Anlysis and implementation of mAP-evaluation of object detection","slug":"map-evaluation of object detection","date":"2019-10-13T06:40:27.000Z","updated":"2020-08-20T13:26:34.654Z","comments":true,"path":"2019/10/13/map-evaluation of object detection/","link":"","permalink":"http://yoursite.com/2019/10/13/map-evaluation of object detection/","excerpt":"","text":"Abstract​ 目标检测中衡量识别精度的常用指标是mAP（mean average precision）。多个类别物体检测中，每一个类别都可以根据recall和precision绘制一条曲线，AP就是该曲线下的面积，mAP是多个类别AP的平均值 。 InformationCode: https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocotools/cocoeval.py https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/datasets/voc_eval.py Reference: 目标检测中的mAP是什么含义？ - Wentao的回答 - 知乎https://www.zhihu.com/question/53405779/answer/419532990 ROC，AUC，PR，AP介绍及python绘制 https://www.cnblogs.com/zf-blog/p/6734686.html The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Development Kit 用mAP衡量目标检测的性能是否科学？ - Angzz的回答 - 知乎 https://www.zhihu.com/question/337856533/answer/769453722 Approach DescriptionmAP定义集相关概念 mAP: mean Average Precision, 即各类别AP的平均值 AP: PR曲线下的面积 PR曲线: Precision-Recall曲线 Precision: TP / (TP + FP) Recall: TP / (TP + FN) TP: IoU&gt;0.5的检测框数量（同一Ground Truth只计算一次） FP: IoU&lt;=0.5的检测框，或者是检测到同一个GT的多余检测框的数量 （ps:多余的意思是指对一个gt预测出多个框的iou大于阈值，只考虑最大iou的 检测框） FN: 没有检测到的GT的数量 mAP的计算方法​ 要计算mAP必须先绘出各类别PR曲线，通过对PR曲线的面积统计得出AP。 ​ 如何采样PR曲线，VOC，COCO都采用过下面几种不同方法。 在VOC2010以前，只需要选取当Recall &gt;= 0, 0.1, 0.2, …, 1共11个点时的Precision最大值，然后AP就是这11个Precision的平均值。 在VOC2010及以后， 假设设这一张图片M个正例，那么我们会得到M个recall值（1/M, 2/M, …, M/M）,对于每个recall值r，我们可以计算出对应（r’ &gt; r）的最大precision，然后对这M个precision值取平均即得到最后的AP值。 COCO(pycocotools) 选取当Recall &gt;= 0, 0.01, 0.02, …, 1共101个点时的Precision最大值，然后AP就是这101个Precision的平均值。 如何计算Precision, Recall: 根据Precision的定义需要确定TP、FP、FN的值，那么如何确定这些值并且和设定recall阈值对应起来呢？ 确定iou_thres(0.5)，将pred_boxes按score排序，相同imageID下，将pred_boxes 和gt_boxes进行匹配： (method1) 对每个gt_box 计算与未被匹配的pred_boxes的iou，取最大iou的pred_box，若iou&gt;iou_thres则该pred_box置为TP,否则为FP (method2，voc)对每个pred_box 计算与gt_boxes的iou,取最大iou的gt_box，若iou&gt;iou_thres且该gt_box未被匹配则将该pred_box置为TP，否则为FP 经过上述方法可以得到长度为len(pred_boxes)的tps/fps数组，可以用1代表对应位置为TP/FP 对tps，fps分别进行累加，得到累加数组，第i个元素代表在第i个pred_boxes的score划分正负样本时，TP和FP的值（ps：只考虑了预测为正样本部分），于是可以得到不同score下的Precision，Recall的值，分别记为precisions,recalls。Recall的分母为len(gt_boxes) 平滑处理： 对precisions的每个元素，取当前位置后面所有precision的最大值 根据确定的Recall threshold找到其在Recall的位置，并确定其precision值，相邻的recall相减并与precision值相乘，所有乘积之和就是ap note: ​ precision的最大值体现在如果只是简单取recall阈值下的precision那么绘制出来的PR曲线并不是单调递减的。 举例来说tps为 [0,1,0,1,1]，则累加后为：[0,1,1,2,3]，precisions为：[0,1/2,1/3/，2/4，3/5]，可以看到precision可能是波动的，PR曲线并不是单调递减的，通过平滑处理，可以保证PR曲线的单调性，有利于观察PR曲线选择合适的score阈值。 参考第2个链接的例子理解上述过程： 假设从测试集中共检测出20个例子，而测试集中共有6个正例，则PR表如下： PR波形图如下 Codepseudo code：给出一张图片，一个类别时的AP伪代码 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as np# Box = Tuple[int, int, int, int]# [left, top, right, bottom]def IoU(box1:Box, box2:Box) -&gt; float: pass def AP(boxes:List[Box], gt_boxes:List[Box], thres:float) -&gt; float: N = len(boxes) M = len(gt_boxes) pred_match = [0 for i in range(N)] for i in range(M): max_id = 0 max_iou = 0 for j in range(N): if pred_match[j]==0: iou = IoU(gt_boxes[i],boxes[j]) if iou&gt;thres: if iou&gt;max_iou: max_id = j max_iou = iou pred_match[max_id] = i+1 precisions = [] # recalls = [] tp = 0 for i in range(N): if pred_match[i]!=0: tp += (pred_match[i]!=0) fp = N-tp recall = tp/M precision = tp/(i+1) # recalls.append(recall) precisions.append(precision) # smooth precisions = [0.0]+precisions+[0.0] for i in range(len(precisions)-1,0,-1): precisions[i-1] = max(precisions[i-1],precisions[i]) precisions = precisions[1:-1] ap = [precisions[i]*1/M for i in range(len(precisions))] ap = sum(ap) return ap Extension:​ 在训练好detection model，可以通过softnms，多尺度测试，flip测试增强等trick提升mAP, 但是基本都是通过提升recall，涨的低Precision的区域，低精度区对应用场景来说没用 。 ​ 实际应用中可以使用FPPI(False Positve Per Image), MISS Rate(行人检测常用)","categories":[{"name":"Ojbect_Detection","slug":"Ojbect-Detection","permalink":"http://yoursite.com/categories/Ojbect-Detection/"}],"tags":[{"name":"Ojbect_Detection","slug":"Ojbect-Detection","permalink":"http://yoursite.com/tags/Ojbect-Detection/"}]},{"title":"Reranking","slug":"Reranking","date":"2019-09-17T16:40:27.000Z","updated":"2020-08-20T13:26:34.649Z","comments":true,"path":"2019/09/18/Reranking/","link":"","permalink":"http://yoursite.com/2019/09/18/Reranking/","excerpt":"","text":"Abstract​ 在图像检索过程中，或者ReID(将 Re-ID看作一个检索过程时)，re-ranking是提高其准确性的关键步骤，是各大竞赛刷榜得利器。 ​ Re-Ranking的思想基于这么一个假设： ​ if a gallery image is similar to the probe in the k-reciprocal nearest neighbors, it is more likely to be a true match. ​ 简单的解释就是，根据probe在probes+galleries搜索出来的candidate对象，根据这些candidate对象在probes+galleries选择k个nearest，如果包含你的probe，那它的可能性更大一些。 InformationPaper: (CVPR2017) Re-ranking Person Re-identification with k-reciprocal Encoding. Code: (matlab) https://github.com/zhunzhong07/person-re-ranking (python cpu:代码简洁，使用堆排序加速，k reciprocal nearest neighbors没有去重) https://github.com/layumi/Person_reID_baseline_pytorch/ (pytorch gpu) https://github.com/michuanhaohao/reid-strong-baseline Reference: https://blog.csdn.net/lwplwf/article/details/84862054 谁能解释下 行人重识别reranking的原理？ - linolzhang的回答 - 知乎https://www.zhihu.com/question/271308170/answer/361943914 https://zhuanlan.zhihu.com/p/36834023 Approach DescriptionK-reciprocal Nearest Neighbors​ 回顾常规检索过程，probe(p)在galleries检索的topk({g1,g2,..,}),定义为： ​ k-reciprocal nearest neighbors定义为： ​ probe(p)在probes+galleries检索的topk的candidates对象（不包含p），如果这些candidates对象在probes+galleries选泽的topk包含了p,则该candidate与p互为top-k。 ​ 有时候probe的匹配图片不在probe的top-k之中，这时候使用下面的方法进行召回： ​ 简单的解释就是，对任一q属于k-reciprocal nearest neightbors, 求R(q,k/2)； 若R(q,k/2)与R(p,k)的交集数量大于2/3的|R(q,k/2)|，则将R(q,k/2)合并到R(p,k)，得到expanded k-reciprocalnearest neightbors. Jaccard Distance​ 若probe和gallery的两个图片的k-reciprocal nearest neighbor重叠的越多，可以认为两张图片越近似： ​ 可以使用该距离做为新的度量方式，但是该方法存在几个缺点： （1）时间复杂度高： 重新计算距离矩阵的复杂度为$$O((N+M)^2kk)$$N,M分别为probes, galleries的图片数量 计算expanded k-reciprocalnearest neightbors的时间复杂度为：$$O((N+M)(D(N+M)+(N+M)+klog(N+M)+kk/2log(N+M)))$$其中, D为feature的长度，(N+M)是建堆的时间复杂度，kxlog(N+M)是topk的查询复杂度，kxk/2xlog(N+M))是expand的时间复杂度。 ps: 其实个人理解使用修改后的方案（k-reciprocal feature），还是要计算expanded k-reciprocalnearest neightbors，所以k-reciprocal feature并不是针对这点提出的改进。 （2）jaccard distance未考虑top-k中每张图片的权重，但显然排名高的图像权重应该更大 考虑将k-reciprocal nearest neighbor编码为向量，称为k-reciprocal feature k-reciprocal feature 即对属于expand k-reciprocal nearest neighbor的编码为1，否则为0，长度为probes+galleries的长度. 接下来，对编码为1的使用指数函数计算相似度： 其中公式（8）（9）可以这样理解，对于一个向量，按元素操作，0和非零值取min代表着交集，0和非零值取max 代表着并集，其一阶范数代表这集合得大小。 Local Query Expansion假设相同ID的图片拥有相似的特征，因此可以使用probe的topk对Vp进行修正，注意top-k应该包含了p： Codereranking 主要有3个参数： k1=20, k2=6, lambda_value=0.3 k1是计算k-reciprocal nearest neighbor的k，不宜过大，特别是存在大量只有几张图片的ID时 k2是计算Local Query Expansion的k, 不宜过大，特别是存在大量只有几张图片的ID时，而且一般比k1小","categories":[{"name":"ReID","slug":"ReID","permalink":"http://yoursite.com/categories/ReID/"}],"tags":[{"name":"ReID","slug":"ReID","permalink":"http://yoursite.com/tags/ReID/"}]}]}